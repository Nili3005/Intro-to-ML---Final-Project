{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nili3005/ML/blob/main/Project_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ngboost\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEiQTmeWako1",
        "outputId": "540d5e87-2f79-47ad-cf30-21d2b2001b00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ngboost\n",
            "  Downloading ngboost-0.4.2-py3-none-any.whl (33 kB)\n",
            "Collecting lifelines>=0.25 (from ngboost)\n",
            "  Downloading lifelines-0.27.8-py3-none-any.whl (350 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.7/350.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from ngboost) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.10/dist-packages (from ngboost) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lifelines>=0.25->ngboost) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines>=0.25->ngboost) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines>=0.25->ngboost) (1.6.2)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n",
            "  Downloading formulaic-0.6.6-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->ngboost) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->ngboost) (3.2.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines>=0.25->ngboost) (0.18.3)\n",
            "Collecting astor>=0.8 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->lifelines>=0.25->ngboost) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.16.0)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=17d226582c4f49ff49bba8f804567dd527ca00b9cc5f0cf7d12a78d340048fd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cc/e0/ef2969164144c899fedb22b338f6703e2b9cf46eeebf254991\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, astor, autograd-gamma, formulaic, lifelines, ngboost\n",
            "Successfully installed astor-0.8.1 autograd-gamma-0.5.0 formulaic-0.6.6 interface-meta-1.3.0 lifelines-0.27.8 ngboost-0.4.2\n",
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v68EV_fYGBt"
      },
      "source": [
        "# Usage\n",
        "\n",
        "We'll start with a probabilistic regression example on the Boston housing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "remove_cell"
        ],
        "id": "B69cHiqVYGBw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/Users/c242587/Desktop/projects/git/ngboost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wF7W2EDIYGBx",
        "outputId": "f3e06b44-b08b-45f7-d3df-d36c0f21ae38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5ad595a4127a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \"\"\"\n\u001b[1;32m    155\u001b[0m         )\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ngboost import NGBRegressor\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "ngb = NGBRegressor().fit(X_train, Y_train)\n",
        "Y_preds = ngb.predict(X_test)\n",
        "Y_dists = ngb.pred_dist(X_test)\n",
        "\n",
        "# test Mean Squared Error\n",
        "test_MSE = mean_squared_error(Y_preds, Y_test)\n",
        "print('Test MSE', test_MSE)\n",
        "\n",
        "# test Negative Log Likelihood\n",
        "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
        "print('Test NLL', test_NLL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfLWEN1-YGBy"
      },
      "source": [
        "Getting the estimated distributional parameters at a set of points is easy. This returns the predicted mean and standard deviation of the first five observations in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhCybQf8YGBz"
      },
      "outputs": [],
      "source": [
        "Y_dists[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNBbqxuMYGBz"
      },
      "source": [
        "## Distributions\n",
        "\n",
        "NGBoost can be used with a variety of distributions, broken down into those for regression (support on an infinite set) and those for classification (support on a finite set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkmsiM1JYGB0"
      },
      "source": [
        "### Regression Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebYJtImvYGB0"
      },
      "source": [
        "| Distribution | Parameters | Implemented Scores | Reference |\n",
        "| --- | --- | --- | --- |\n",
        "| `Normal` | `loc`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) |\n",
        "| `LogNormal` | `s`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` lognormal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html) |\n",
        "| `Exponential` | `scale` | `LogScore`, `CRPScore` | [`scipy.stats` exponential](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUp6rX89YGB1"
      },
      "source": [
        "Regression distributions can be used through the `NGBRegressor()` constructor by passing the appropriate class as the `Dist` argument. `Normal` is the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzD8-SUVYGB1"
      },
      "outputs": [],
      "source": [
        "from ngboost.distns import Exponential, Normal\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_reg_train, X_reg_test, Y_reg_train, Y_reg_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "ngb_norm = NGBRegressor(Dist=Normal, verbose=False).fit(X_reg_train, Y_reg_train)\n",
        "ngb_exp = NGBRegressor(Dist=Exponential, verbose=False).fit(X_reg_train, Y_reg_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXJ8b0aYGB1"
      },
      "source": [
        "There are two prediction methods for `NGBRegressor` objects: `predict()`, which returns point predictions as one would expect from a standard regressor, and `pred_dist()`, which returns a distribution object representing the conditional distribution of $Y|X=x_i$ at the points $x_i$ in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYWitZ9OYGB1"
      },
      "outputs": [],
      "source": [
        "ngb_norm.predict(X_reg_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HgrqhoMYGB2"
      },
      "outputs": [],
      "source": [
        "ngb_exp.predict(X_reg_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDlzrWNYGB2"
      },
      "outputs": [],
      "source": [
        "ngb_exp.pred_dist(X_reg_test)[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlBmH1KYGB2"
      },
      "source": [
        "#### Survival Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn7FxIO6YGB3"
      },
      "source": [
        "NGBoost supports analyses of right-censored data. Any distribution that can be used for regression in NGBoost can also be used for survival analysis in theory, but this requires the implementation of the right-censored version of the appropriate score. At the moment, `LogNormal` and `Exponential` have these scores implemented. To do survival analysis, use `NGBSurvival` and pass both the time-to-event (or censoring) and event indicator vectors to  `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byYiYJ5jYGB3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ngboost import NGBSurvival\n",
        "from ngboost.distns import LogNormal\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_surv_train, X_surv_test, Y_surv_train, Y_surv_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "# introduce administrative censoring to simulate survival data\n",
        "T_surv_train = np.minimum(Y_surv_train, 30) # time of an event or censoring\n",
        "E_surv_train = Y_surv_train > 30 # 1 if T[i] is the time of an event, 0 if it's a time of censoring\n",
        "\n",
        "ngb = NGBSurvival(Dist=LogNormal).fit(X_surv_train, T_surv_train, E_surv_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rvKBR02YGB3"
      },
      "source": [
        "The scores currently implemented assume that the censoring is independent of survival, conditional on the observed predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KnXB22YGB3"
      },
      "source": [
        "### Classification Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv67RKIFYGB3"
      },
      "source": [
        "| Distribution | Parameters | Implemented Scores | Reference |\n",
        "| --- | --- | --- | --- |\n",
        "| `k_categorical(K)` | `p0`, `p1`... `p{K-1}` | `LogScore` | [Categorical distribution on Wikipedia](https://en.wikipedia.org/wiki/Categorical_distribution) |\n",
        "| `Bernoulli` | `p` | `LogScore` | [Bernoulli distribution on Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEgUThdYGB3"
      },
      "source": [
        "Classification distributions can be used through the `NGBClassifier()` constructor by passing the appropriate class as the `Dist` argument. `Bernoulli` is the default and is equivalent to `k_categorical(2)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o__ZxDBFYGB3"
      },
      "outputs": [],
      "source": [
        "from ngboost import NGBClassifier\n",
        "from ngboost.distns import k_categorical, Bernoulli\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(True)\n",
        "y[0:15] = 2 # artificially make this a 3-class problem instead of a 2-class problem\n",
        "X_cls_train, X_cls_test, Y_cls_train, Y_cls_test  = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "ngb_cat = NGBClassifier(Dist=k_categorical(3), verbose=False) # tell ngboost that there are 3 possible outcomes\n",
        "_ = ngb_cat.fit(X_cls_train, Y_cls_train) # Y should have only 3 values: {0,1,2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdD9ivqlYGB4"
      },
      "source": [
        "When using NGBoost for classification, the outcome vector `Y` must consist only of integers from 0 to K-1, where K is the total number of classes. This is consistent with the classification standards in sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FJRaHbGYGB4"
      },
      "source": [
        "`NGBClassifier` objects have three prediction methods: `predict()` returns the most likely class, `predict_proba()` returns the class probabilities, and `pred_dist()` returns the distribution object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dajpHilnYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.predict(X_cls_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Gm328uYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.predict_proba(X_cls_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RILuySxqYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.pred_dist(X_cls_test)[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgVVFFOQYGB4"
      },
      "source": [
        "## Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEiDd5bYGB4"
      },
      "source": [
        "NGBoost supports the log score (`LogScore`, also known as negative log-likelihood) and CRPS (`CRPScore`), although each score may not be implemented for each distribution. The score is specified by the `Score` argument in the constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "KCb4GcT3YGB4"
      },
      "outputs": [],
      "source": [
        "from ngboost.scores import LogScore, CRPScore\n",
        "\n",
        "NGBRegressor(Dist=Exponential, Score=CRPScore, verbose=False).fit(X_reg_train, Y_reg_train)\n",
        "NGBClassifier(Dist=k_categorical(3), Score=LogScore, verbose=False).fit(X_cls_train, Y_cls_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuWNTQVIYGB5"
      },
      "source": [
        "## Base Learners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGbzOC7YGB5"
      },
      "source": [
        "NGBoost can be used with any sklearn regressor as the base learner, specified with the `Base` argument. The default is a depth-3 regression tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "-6L6fzDPYGB5"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "learner = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
        "\n",
        "NGBSurvival(Dist=Exponential, Score=CRPScore, Base=learner, verbose=False).fit(X_surv_train, T_surv_train, E_surv_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI-jzSz8YGB5"
      },
      "source": [
        "## Other Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYJc7c4gYGB5"
      },
      "source": [
        "The learning rate, number of estimators, minibatch fraction, and column subsampling are also easily adjusted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "3r-naJj-YGB5"
      },
      "outputs": [],
      "source": [
        "ngb = NGBRegressor(n_estimators=100, learning_rate=0.01,\n",
        "             minibatch_frac=0.5, col_sample=0.5)\n",
        "ngb.fit(X_reg_train, Y_reg_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzIKH4epYGB5"
      },
      "source": [
        "Sample weights (for training) are set using the `sample_weight` argument to `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "WOudBxY5YGB6"
      },
      "outputs": [],
      "source": [
        "ngb = NGBRegressor(n_estimators=100, learning_rate=0.01,\n",
        "             minibatch_frac=0.5, col_sample=0.5)\n",
        "weights = np.random.random(Y_reg_train.shape)\n",
        "ngb.fit(X_reg_train, Y_reg_train, sample_weight=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wZ_KQgKYGB6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}