{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nili3005/ML/blob/main/Project_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v68EV_fYGBt"
      },
      "source": [
        "# Usage\n",
        "\n",
        "We'll start with a probabilistic regression example on the Boston housing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "remove_cell"
        ],
        "id": "B69cHiqVYGBw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/Users/c242587/Desktop/projects/git/ngboost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wF7W2EDIYGBx",
        "outputId": "06312f81-9c4b-4d5a-b7e0-f28265a139da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5ad595a4127a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ngboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ngboost import NGBRegressor\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "ngb = NGBRegressor().fit(X_train, Y_train)\n",
        "Y_preds = ngb.predict(X_test)\n",
        "Y_dists = ngb.pred_dist(X_test)\n",
        "\n",
        "# test Mean Squared Error\n",
        "test_MSE = mean_squared_error(Y_preds, Y_test)\n",
        "print('Test MSE', test_MSE)\n",
        "\n",
        "# test Negative Log Likelihood\n",
        "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
        "print('Test NLL', test_NLL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfLWEN1-YGBy"
      },
      "source": [
        "Getting the estimated distributional parameters at a set of points is easy. This returns the predicted mean and standard deviation of the first five observations in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhCybQf8YGBz"
      },
      "outputs": [],
      "source": [
        "Y_dists[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNBbqxuMYGBz"
      },
      "source": [
        "## Distributions\n",
        "\n",
        "NGBoost can be used with a variety of distributions, broken down into those for regression (support on an infinite set) and those for classification (support on a finite set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkmsiM1JYGB0"
      },
      "source": [
        "### Regression Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebYJtImvYGB0"
      },
      "source": [
        "| Distribution | Parameters | Implemented Scores | Reference |\n",
        "| --- | --- | --- | --- |\n",
        "| `Normal` | `loc`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) |\n",
        "| `LogNormal` | `s`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` lognormal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html) |\n",
        "| `Exponential` | `scale` | `LogScore`, `CRPScore` | [`scipy.stats` exponential](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUp6rX89YGB1"
      },
      "source": [
        "Regression distributions can be used through the `NGBRegressor()` constructor by passing the appropriate class as the `Dist` argument. `Normal` is the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzD8-SUVYGB1"
      },
      "outputs": [],
      "source": [
        "from ngboost.distns import Exponential, Normal\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_reg_train, X_reg_test, Y_reg_train, Y_reg_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "ngb_norm = NGBRegressor(Dist=Normal, verbose=False).fit(X_reg_train, Y_reg_train)\n",
        "ngb_exp = NGBRegressor(Dist=Exponential, verbose=False).fit(X_reg_train, Y_reg_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXJ8b0aYGB1"
      },
      "source": [
        "There are two prediction methods for `NGBRegressor` objects: `predict()`, which returns point predictions as one would expect from a standard regressor, and `pred_dist()`, which returns a distribution object representing the conditional distribution of $Y|X=x_i$ at the points $x_i$ in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYWitZ9OYGB1"
      },
      "outputs": [],
      "source": [
        "ngb_norm.predict(X_reg_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HgrqhoMYGB2"
      },
      "outputs": [],
      "source": [
        "ngb_exp.predict(X_reg_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDlzrWNYGB2"
      },
      "outputs": [],
      "source": [
        "ngb_exp.pred_dist(X_reg_test)[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlBmH1KYGB2"
      },
      "source": [
        "#### Survival Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn7FxIO6YGB3"
      },
      "source": [
        "NGBoost supports analyses of right-censored data. Any distribution that can be used for regression in NGBoost can also be used for survival analysis in theory, but this requires the implementation of the right-censored version of the appropriate score. At the moment, `LogNormal` and `Exponential` have these scores implemented. To do survival analysis, use `NGBSurvival` and pass both the time-to-event (or censoring) and event indicator vectors to  `fit()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byYiYJ5jYGB3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ngboost import NGBSurvival\n",
        "from ngboost.distns import LogNormal\n",
        "\n",
        "X, Y = load_boston(True)\n",
        "X_surv_train, X_surv_test, Y_surv_train, Y_surv_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "# introduce administrative censoring to simulate survival data\n",
        "T_surv_train = np.minimum(Y_surv_train, 30) # time of an event or censoring\n",
        "E_surv_train = Y_surv_train > 30 # 1 if T[i] is the time of an event, 0 if it's a time of censoring\n",
        "\n",
        "ngb = NGBSurvival(Dist=LogNormal).fit(X_surv_train, T_surv_train, E_surv_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rvKBR02YGB3"
      },
      "source": [
        "The scores currently implemented assume that the censoring is independent of survival, conditional on the observed predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KnXB22YGB3"
      },
      "source": [
        "### Classification Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv67RKIFYGB3"
      },
      "source": [
        "| Distribution | Parameters | Implemented Scores | Reference |\n",
        "| --- | --- | --- | --- |\n",
        "| `k_categorical(K)` | `p0`, `p1`... `p{K-1}` | `LogScore` | [Categorical distribution on Wikipedia](https://en.wikipedia.org/wiki/Categorical_distribution) |\n",
        "| `Bernoulli` | `p` | `LogScore` | [Bernoulli distribution on Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEgUThdYGB3"
      },
      "source": [
        "Classification distributions can be used through the `NGBClassifier()` constructor by passing the appropriate class as the `Dist` argument. `Bernoulli` is the default and is equivalent to `k_categorical(2)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o__ZxDBFYGB3"
      },
      "outputs": [],
      "source": [
        "from ngboost import NGBClassifier\n",
        "from ngboost.distns import k_categorical, Bernoulli\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(True)\n",
        "y[0:15] = 2 # artificially make this a 3-class problem instead of a 2-class problem\n",
        "X_cls_train, X_cls_test, Y_cls_train, Y_cls_test  = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "ngb_cat = NGBClassifier(Dist=k_categorical(3), verbose=False) # tell ngboost that there are 3 possible outcomes\n",
        "_ = ngb_cat.fit(X_cls_train, Y_cls_train) # Y should have only 3 values: {0,1,2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdD9ivqlYGB4"
      },
      "source": [
        "When using NGBoost for classification, the outcome vector `Y` must consist only of integers from 0 to K-1, where K is the total number of classes. This is consistent with the classification standards in sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FJRaHbGYGB4"
      },
      "source": [
        "`NGBClassifier` objects have three prediction methods: `predict()` returns the most likely class, `predict_proba()` returns the class probabilities, and `pred_dist()` returns the distribution object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dajpHilnYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.predict(X_cls_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Gm328uYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.predict_proba(X_cls_test)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RILuySxqYGB4"
      },
      "outputs": [],
      "source": [
        "ngb_cat.pred_dist(X_cls_test)[0:5].params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgVVFFOQYGB4"
      },
      "source": [
        "## Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEiDd5bYGB4"
      },
      "source": [
        "NGBoost supports the log score (`LogScore`, also known as negative log-likelihood) and CRPS (`CRPScore`), although each score may not be implemented for each distribution. The score is specified by the `Score` argument in the constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "KCb4GcT3YGB4"
      },
      "outputs": [],
      "source": [
        "from ngboost.scores import LogScore, CRPScore\n",
        "\n",
        "NGBRegressor(Dist=Exponential, Score=CRPScore, verbose=False).fit(X_reg_train, Y_reg_train)\n",
        "NGBClassifier(Dist=k_categorical(3), Score=LogScore, verbose=False).fit(X_cls_train, Y_cls_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuWNTQVIYGB5"
      },
      "source": [
        "## Base Learners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGbzOC7YGB5"
      },
      "source": [
        "NGBoost can be used with any sklearn regressor as the base learner, specified with the `Base` argument. The default is a depth-3 regression tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "-6L6fzDPYGB5"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "learner = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
        "\n",
        "NGBSurvival(Dist=Exponential, Score=CRPScore, Base=learner, verbose=False).fit(X_surv_train, T_surv_train, E_surv_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI-jzSz8YGB5"
      },
      "source": [
        "## Other Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYJc7c4gYGB5"
      },
      "source": [
        "The learning rate, number of estimators, minibatch fraction, and column subsampling are also easily adjusted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "3r-naJj-YGB5"
      },
      "outputs": [],
      "source": [
        "ngb = NGBRegressor(n_estimators=100, learning_rate=0.01,\n",
        "             minibatch_frac=0.5, col_sample=0.5)\n",
        "ngb.fit(X_reg_train, Y_reg_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzIKH4epYGB5"
      },
      "source": [
        "Sample weights (for training) are set using the `sample_weight` argument to `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide_output"
        ],
        "id": "WOudBxY5YGB6"
      },
      "outputs": [],
      "source": [
        "ngb = NGBRegressor(n_estimators=100, learning_rate=0.01,\n",
        "             minibatch_frac=0.5, col_sample=0.5)\n",
        "weights = np.random.random(Y_reg_train.shape)\n",
        "ngb.fit(X_reg_train, Y_reg_train, sample_weight=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wZ_KQgKYGB6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}